{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc779c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-core:3.2.0`\n",
    "import $ivy.`org.apache.spark::spark-sql:3.2.0`\n",
    "import $ivy.`org.apache.spark::spark-hive:3.2.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b341135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{LogManager, Level}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => F}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@568d52ce\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{LogManager, Level}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.{functions => F}\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "LogManager.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "val spark = SparkSession.builder.\n",
    "    appName(\"app123\").\n",
    "    master(\"local\").\n",
    "    config(\"spark.sql.warehouse.dir\",\"/user/hive/warehouse\").\n",
    "    config(\"spark.hadoop.fs.default.name\", \"hdfs://localhost:9000\").\n",
    "    enableHiveSupport().\n",
    "    getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f681ba3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+-----+------+---------+---+-----+\n",
      "|   id|time|pass|fname| lname|    sname|inn|snils|\n",
      "+-----+----+----+-----+------+---------+---+-----+\n",
      "|12345|   1| 123| ivan|ivanov|     null|111|  000|\n",
      "|12345|   2| 123| ivan|ivanov|     null|111|  000|\n",
      "|12345|   3| 123| ivan|ivanov|ivanovich|111|     |\n",
      "|12345|   4| 123| ivan|ivanov|     null|   |  000|\n",
      "+-----+----+----+-----+------+---------+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [id: int, time: int ... 6 more fields]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "    (12345, 1, \"123\", \"ivan\", \"ivanov\", null, \"111\", \"000\"),\n",
    "    (12345, 2, \"123\", \"ivan\", \"ivanov\", null, \"111\", \"000\"),\n",
    "    (12345, 3, \"123\", \"ivan\", \"ivanov\", \"ivanovich\", \"111\", \"\"),\n",
    "    (12345, 4, \"123\", \"ivan\", \"ivanov\", null, \"\", \"000\"),\n",
    "   \n",
    "   ).\n",
    "    toDF(\"id\", \"time\", \"pass\", \"fname\", \"lname\", \"sname\", \"inn\", \"snils\")\n",
    "\n",
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff520d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+-----+------+---------+---------+---------+\n",
      "|   id|time|pass|fname| lname|    sname| fforward|    fback|\n",
      "+-----+----+----+-----+------+---------+---------+---------+\n",
      "|12345|   1| 123| ivan|ivanov|     null|     null|ivanovich|\n",
      "|12345|   2| 123| ivan|ivanov|     null|     null|ivanovich|\n",
      "|12345|   3| 123| ivan|ivanov|ivanovich|ivanovich|ivanovich|\n",
      "|12345|   4| 123| ivan|ivanov|     null|ivanovich|ivanovich|\n",
      "+-----+----+----+-----+------+---------+---------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mwf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@79f765c9\n",
       "\u001b[36mwb\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mexpressions\u001b[39m.\u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@4f6fe8a1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wf = Window.\n",
    "    partitionBy(\"id\",\"pass\").\n",
    "    orderBy(\"time\").\n",
    "    rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "val wb = Window.\n",
    "    partitionBy(\"id\", \"pass\").\n",
    "    orderBy(\"time\").\n",
    "    rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    "\n",
    "\n",
    "df.\n",
    "    select(\"id\",\"time\",\"pass\",\"fname\",\"lname\", \"sname\").\n",
    "    withColumn(\"fforward\", F.last(\"sname\", true).over(wf)).\n",
    "    withColumn(\"fback\", F.first(\"fforward\", true).over(wb)).show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala_2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
